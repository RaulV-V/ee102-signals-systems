\makeatletter
\def\input@path{{../styles/}{../../styles/}{../../../styles/}{../}{../../}{../../../}}
\makeatother


\documentclass{ee102_pset} % if your class lives outside
\input{macros.tex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{listings}

% Assignment info
\author{\rule{3cm}{0.4pt}} % Name placeholder
\submitdate{\rule{3cm}{0.4pt}} % Submission date placeholder
\problemset{Pre-requisite \#1: Vectors and Linear Algebra}
\renewcommand{\duedate}{August 31, 2025}
\shorttitle{Pre-requisite \#1}

\begin{document}

% Problem 1
\problem{1} Recall the concepts of linear independence, unit vectors, and orthogonality to answer this problem. Note that although you may be used to using the $\vec{v}$ notation, in EE 102, we will use boldface notation for vectors (e.g., \textbf{v}).
Let $\textbf{a} = [2, -1]$, $\textbf{b} = [1, 3]$, and $\textbf{c} = [4, 0]$.
\problempart \textbf{[6 pts]} Find a unit vector in the direction of $\textbf{a}$. \\ 
\textit{Hint: Remember that a unit vector has a magnitude of 1.}
\problempart \textbf{[7 pts]} Find a vector orthogonal to both $\textbf{a}$ and $\textbf{b}$ in $\mathbb{R}^3$. \\ \textit{Hint: A vector in $\mathbb{R}^3$ has three components, so for this problem you may expand $\textbf{a}$ and $\textbf{b}$ with 0 for the third coordinate. Orthogonality of vectors $\textbf{u}$ and $\textbf{v}$ means that their dot product $\textbf{u} \cdot \textbf{v} = 0$.}
\problempart \textbf{[7 pts]} Prove that $\textbf{a}$ and $\textbf{b}$ are linearly independent. Explain what you understand by linear dependence and independence of vectors.\\ \textit{Hint: Is there a constant that you can multiply $\textbf{a}$ by to get $\textbf{b}$? How does that help with establishing linear independence?}
\vspace*{\fill}
\begin{center}
[use more pages if needed]
\end{center}




% Problem 2
\problem{2} Show your understanding of vectors in 3D! 
Let $\textbf{u} = [1, 2, 3]$, $\textbf{v} = [4, 5, 6]$, $\textbf{w} = [7, 8, 9]$.
\problempart \textbf{[10 pts]} Are $\textbf{u}$, $\textbf{v}$, and $\textbf{w}$ linearly dependent? Justify your answer numerically. Explain your intuition.
\problempart \textbf{[10 pts]} Express $\textbf{w}$ as a linear combination of $\textbf{u}$ and $\textbf{v}$. Your first step should be to check whether it is even possible to do so.



\vspace*{\fill}
\begin{center}
[use more pages if needed]
\end{center}

% Problem 3
\problem{3} You can stack the vectors by column (or by rows) from previous problems to create matrices. This is a common practice to make the analysis easier: imagine working with 100 different variables vs just one variable for the matrix but size $100 \times 3$ (if you are stacking 3D vectors in columns).
For this problem, consider the matrix $A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix}$.
\problempart \textbf{[10 pts]} Are the columns of $A$ linearly independent? Show your work.
\problempart \textbf{[10 pts]} Find the rank of $A$ and interpret its meaning in terms of independence.

\vspace*{\fill}
\begin{center}
[use more pages if needed]
\end{center}




% Problem 4
\problem{4} With this problem, you will take a step further into vectors and linear algebra by exploring \textbf{projections} and \textbf{basis transformations}. Although these topics might sound confusing, they represent simple intuitions. Think of orthogonal vectors as directions that give you ``maximum degree of freedom'' to choose from. If you have two people who are really similar to each other, their thoughts/outcomes will be correlated and you will not gain much by listening to both of them. However, if they are very different (orthogonal), you are likely to gain much more by combining two ``orthogonal'' perspectives. Basis vectors is related to this concept. Basis vectors are a set of vectors that are linearly independent and span the entire space (a set of people who will give you all the perspective there is to get in this world!). Explore this with two vectors in $\mathbb{R}^2$, we have $\textbf{x} = [3, 4]$.
\problempart \textbf{[7 pts]} Find the orthogonal projection of $\textbf{x}$ onto the $x$-axis and $y$-axis.
\problempart \textbf{[7 pts]} Propose a new basis $\mathcal{B} = \{[1, 1], [1, -1]\}$ and express $\textbf{x}$ in terms of $\mathcal{B}$.
\problempart \textbf{[6 pts]} Prove that $\mathcal{B}$ is a basis for $\mathbb{R}^2$. That is, all vectors in $\mathbb{R}^2$ can be expressed as a linear combination of the vectors in $\mathcal{B}$.

\vspace*{\fill}
\begin{center}
[use more pages if needed]
\end{center}



% Problem 5
\problem{5}
Use Python (or MATLAB) to verify your answers to the problems above.
\\ \textbf{Instructions:} Create a Jupyter notebook (`.ipynb`; or MATLAB interactive notebook) where you write and run your code for this problem. When finished, print the notebook as a PDF and attach the PDF to your final submission.
\problempart \textbf{[7 pts]} Write code to check linear independence of vectors in Problems 2 and 3.
\problempart \textbf{[7 pts]} Write code to compute the orthogonal projection in Problem 4.
\problempart \textbf{[6 pts]} Write code to express $\vec{x}$ in the new basis $\mathcal{B}$.

\begin{lstlisting}[language=Python, caption=Starter code for Problem 5]
import numpy as np
# Example: Check linear independence
u = np.array([1, 2, 3])
v = np.array([4, 5, 6])
w = np.array([7, 8, 9])
A = np.column_stack([u, v, w])
rank = np.linalg.matrix_rank(A)
print("Rank:", rank) # Explain how this proves linear dependence.
\end{lstlisting}

\end{document}
